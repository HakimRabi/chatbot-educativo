import os
import re
import logging
import sys
import time
import traceback
from glob import glob
from datetime import datetime
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.question_answering import load_qa_chain
from langchain.chains import LLMChain, RetrievalQA
from langchain.memory import ConversationBufferMemory
from langchain_community.vectorstores import FAISS, Chroma
from langchain_community.document_loaders import PyPDFLoader

from config import *
from utils import *
from templates import *

logger = logging.getLogger("ai_system")

class AISystem:
    def __init__(self):
        self.fragmentos = []
        self.documentos = []
        self.using_vector_db = False
        self.using_chroma = False
        self.cadena = None
        self.llm = None
        self.llm_cache = {}  # Cache para mÃºltiples modelos
        self.current_model = None
        self.memory = None
        self.vector_store = None
        self.chroma_error_details = None
        self.faiss_error_details = None
        self.is_initialized = False
        self.data_dir = os.path.dirname(CHROMA_PATH) if CHROMA_PATH else "data"
        
        # No inicializar automÃ¡ticamente, se harÃ¡ desde el servidor
    
    def get_or_create_llm(self, model_name=None):
        """Obtiene o crea una instancia de LLM para el modelo especificado"""
        if model_name is None:
            model_name = DEFAULT_MODEL
            
        # Verificar si el modelo estÃ¡ disponible
        if model_name not in AVAILABLE_MODELS:
            logger.warning(f"Modelo {model_name} no disponible, usando {DEFAULT_MODEL}")
            model_name = DEFAULT_MODEL
            
        # Usar cache para evitar recrear instancias
        if model_name in self.llm_cache:
            logger.debug(f"Usando LLM cacheado para modelo: {model_name}")
            return self.llm_cache[model_name]
            
        try:
            model_config = AVAILABLE_MODELS[model_name]
            logger.info(f"Creando nueva instancia LLM para modelo: {model_name}")
            
            llm_instance = OllamaLLM(
                model=model_config["name"],
                temperature=model_config["temperature"]
            )
            
            # Cachear la instancia
            self.llm_cache[model_name] = llm_instance
            logger.info(f"âœ… LLM {model_name} creado y cacheado")
            
            return llm_instance
            
        except Exception as e:
            logger.error(f"âŒ Error creando LLM para {model_name}: {e}")
            # Fallback al modelo por defecto
            if model_name != DEFAULT_MODEL:
                return self.get_or_create_llm(DEFAULT_MODEL)
            raise
    
    def switch_model(self, model_name):
        """Cambia el modelo activo del sistema"""
        try:
            logger.info(f"Cambiando modelo de {self.current_model} a {model_name}")
            
            # Obtener o crear la instancia del LLM
            new_llm = self.get_or_create_llm(model_name)
            
            # Actualizar el LLM principal
            self.llm = new_llm
            self.current_model = model_name
            
            # Recrear la cadena de RetrievalQA si existe vector store
            if self.vector_store is not None:
                logger.info("Actualizando RetrievalQA con nuevo modelo...")
                retriever = self.vector_store.as_retriever(search_kwargs={"k": 3})
                
                self.cadena = RetrievalQA.from_chain_type(
                    llm=self.llm,
                    chain_type="stuff",
                    retriever=retriever,
                    return_source_documents=True,
                    chain_type_kwargs={"prompt": PROMPT_QA_SIMPLE}
                )
                logger.info(f"âœ… RetrievalQA actualizado con modelo {model_name}")
            
            logger.info(f"âœ… Modelo cambiado exitosamente a: {model_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error cambiando modelo a {model_name}: {e}")
            return False
    
    def get_available_models(self):
        """Retorna la lista de modelos disponibles"""
        return AVAILABLE_MODELS
    
    def get_current_model(self):
        """Retorna el modelo actualmente activo"""
        return self.current_model or DEFAULT_MODEL
    
    def check_chromadb_dependencies(self):
        """Verifica las dependencias necesarias para ChromaDB."""
        try:
            import chromadb
            logger.info(f"ChromaDB version: {chromadb.__version__}")
            
            # Verificar sqlite3
            import sqlite3
            logger.info(f"SQLite3 version: {sqlite3.sqlite_version}")
            
            # Verificar que se puede crear un cliente
            try:
                client = chromadb.Client()
                logger.info("ChromaDB client creation test: SUCCESS")
                return True
            except Exception as client_error:
                logger.error(f"ChromaDB client creation failed: {client_error}")
                return False
                
        except ImportError as e:
            logger.error(f"ChromaDB import failed: {e}")
            return False
        except Exception as e:
            logger.error(f"ChromaDB dependency check failed: {e}")
            return False

    def setup_chroma_vectorstore(self, documents, embeddings, persist_directory):
        """Configura ChromaDB con reutilizaciÃ³n inteligente de persistencia"""
        try:
            import chromadb
            from langchain_chroma import Chroma
            
            logger.info(f"ðŸ” Configurando ChromaDB en: {persist_directory}")
            
            # Crear directorio si no existe
            os.makedirs(persist_directory, exist_ok=True)
            
            # Verificar si existe una base de datos persistida
            try:
                client = chromadb.PersistentClient(path=persist_directory)
                collection_name = "langchain"
                
                # API actualizada para ChromaDB v0.6.0+
                try:
                    # Intentar obtener la colecciÃ³n directamente
                    collection = client.get_collection(collection_name)
                    doc_count = collection.count()
                    
                    if doc_count > 0 and doc_count >= len(documents):
                        logger.info(f"ðŸ”„ Reutilizando ChromaDB existente con {doc_count} documentos")
                        
                        # Cargar vector store existente SIN recrear
                        vector_store = Chroma(
                            client=client,
                            collection_name=collection_name,
                            embedding_function=embeddings,
                            persist_directory=persist_directory
                        )
                        
                        logger.info("âœ… ChromaDB cargado desde persistencia - Â¡Inicio rÃ¡pido!")
                        return vector_store
                        
                    elif doc_count > 0 and doc_count < len(documents):
                        logger.info(f"ðŸ“„ Base existente ({doc_count} docs) vs nuevos ({len(documents)} docs)")
                        logger.info("ðŸ”„ Actualizando base de datos...")
                        
                        # Cargar existente y agregar documentos faltantes
                        vector_store = Chroma(
                            client=client,
                            collection_name=collection_name,
                            embedding_function=embeddings,
                            persist_directory=persist_directory
                        )
                        
                        # Agregar documentos nuevos
                        new_docs = documents[doc_count:]
                        if new_docs:
                            vector_store.add_documents(new_docs)
                            logger.info(f"âž• Agregados {len(new_docs)} documentos nuevos")
                        
                        return vector_store
                    else:
                        logger.info("ðŸ“‹ ColecciÃ³n vacÃ­a, recreando...")
                        
                except ValueError as ve:
                    # La colecciÃ³n no existe
                    if "does not exist" in str(ve) or "Collection" in str(ve):
                        logger.info("ðŸ“‹ No existe colecciÃ³n previa, creando nueva...")
                    else:
                        raise ve
                        
            except Exception as e:
                # Si hay error de corrupted database, limpiarla
                if "_type" in str(e) or "corrupted" in str(e).lower():
                    logger.warning(f"ðŸ—‘ï¸ Base de datos corrupta detectada: {e}")
                    logger.info("ðŸ§¹ Limpiando base de datos corrupta...")
                    
                    try:
                        import shutil
                        if os.path.exists(persist_directory):
                            # Hacer backup por seguridad
                            backup_path = f"{persist_directory}_backup_{int(datetime.now().timestamp())}"
                            shutil.move(persist_directory, backup_path)
                            logger.info(f"ðŸ“ Backup creado en: {backup_path}")
                            
                            # Recrear directorio limpio
                            os.makedirs(persist_directory, exist_ok=True)
                            logger.info("âœ… Directorio ChromaDB limpiado")
                        
                    except Exception as cleanup_error:
                        logger.error(f"âŒ Error limpiando DB: {cleanup_error}")
                else:
                    logger.warning(f"âš ï¸ Error verificando base existente: {e}")
                
                logger.info("ðŸ”„ Creando nueva base de datos...")
            
            # Crear nueva base de datos
            logger.info("ðŸ”§ Creando nuevo vector store con ChromaDB...")
            
            vector_store = Chroma.from_documents(
                documents=documents,
                embedding=embeddings,
                persist_directory=persist_directory,
                collection_name=collection_name
            )
            
            logger.info(f"ðŸ’¾ ChromaDB creado con {len(documents)} documentos")
            return vector_store
            
        except Exception as e:
            logger.error(f"âŒ Error configurando ChromaDB: {e}")
            raise

    def setup_vector_database(self):
        """Configura la base de datos vectorial con optimizaciÃ³n de persistencia"""
        try:
            logger.info("ðŸ” Configurando base de datos vectorial...")
            
            if not self.fragmentos:
                logger.warning("âš ï¸ No hay fragmentos disponibles")
                return False
            
            # Configurar embeddings
            embeddings = OllamaEmbeddings(model="nomic-embed-text")
            
            # Configurar ChromaDB con persistencia optimizada
            chroma_path = os.path.join(self.data_dir, "chroma_db")
            
            # Verificar si existe base de datos previa
            if os.path.exists(chroma_path):
                chroma_files = [f for f in os.listdir(chroma_path) if not f.startswith('.')]
                if chroma_files:
                    logger.info(f"ðŸ“ Base de datos ChromaDB existente encontrada ({len(chroma_files)} archivos)")
                    
                    try:
                        # Intentar cargar base existente
                        start_time = time.time()
                        vector_store = self.setup_chroma_vectorstore(
                            self.fragmentos, embeddings, chroma_path
                        )
                        
                        load_time = time.time() - start_time
                        logger.info(f"âš¡ Base de datos cargada en {load_time:.2f} segundos")
                        
                        self.vector_store = vector_store
                        self.using_chroma = True
                        self.using_vector_db = True
                        
                        logger.info("ðŸŽ‰ ChromaDB inicializado con persistencia")
                        return True
                        
                    except Exception as load_error:
                        logger.warning(f"âš ï¸ Error cargando base existente: {load_error}")
                        logger.info("ðŸ”„ Recreando base de datos...")
            
            # Crear nueva base de datos si no existe o fallÃ³ la carga
            logger.info("ðŸ”§ Creando nueva base de datos ChromaDB...")
            start_time = time.time()
            
            vector_store = self.setup_chroma_vectorstore(
                self.fragmentos, embeddings, chroma_path
            )
            
            creation_time = time.time() - start_time
            logger.info(f"âš¡ Base de datos creada en {creation_time:.2f} segundos")
            
            self.vector_store = vector_store
            self.using_chroma = True
            self.using_vector_db = True
            
            logger.info("ðŸŽ‰ ChromaDB creado exitosamente")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error en setup_vector_database: {e}")
            # Continuar con fallback a FAISS si es necesario
            return self.setup_faiss_fallback(embeddings)

    def setup_faiss_fallback(self, embeddings):
        """Configura FAISS como fallback si ChromaDB falla"""
        try:
            logger.info("ðŸ”„ Configurando FAISS como fallback...")
            os.makedirs(FAISS_PATH, exist_ok=True)
            
            self.vector_store = FAISS.from_documents(documents=self.fragmentos, embedding=embeddings)
            self.vector_store.save_local(FAISS_PATH)
            
            self.using_chroma = False
            self.using_vector_db = True
            logger.info("âœ… FAISS configurado como fallback")
            return True
            
        except Exception as faiss_error:
            self.faiss_error_details = {
                "error": str(faiss_error),
                "type": type(faiss_error).__name__,
                "traceback": traceback.format_exc()
            }
            logger.error(f"âŒ Error configurando FAISS: {faiss_error}")
            self.vector_store = None
            self.using_vector_db = False
            return False
    
    def get_error_diagnostics(self):
        """Retorna informaciÃ³n detallada sobre errores ocurridos."""
        diagnostics = {
            "chromadb_available": self.check_chromadb_dependencies(),
            "using_chroma": self.using_chroma,
            "using_vector_db": self.using_vector_db,
            "fragmentos_count": len(self.fragmentos),
            "documentos_count": len(self.documentos),
            "llm_available": self.llm is not None,
            "vector_store_available": self.vector_store is not None,
            "chroma_error": self.chroma_error_details,
            "faiss_error": self.faiss_error_details
        }
        
        # InformaciÃ³n del sistema
        diagnostics["system_info"] = {
            "python_version": sys.version,
            "platform": sys.platform,
            "chroma_path": CHROMA_PATH,
            "faiss_path": FAISS_PATH,
            "chroma_path_exists": os.path.exists(CHROMA_PATH),
            "faiss_path_exists": os.path.exists(FAISS_PATH)
        }
        
        # Verificar versiones especÃ­ficas
        try:
            import chromadb
            diagnostics["chromadb_version"] = chromadb.__version__
        except:
            diagnostics["chromadb_version"] = "No disponible"
        
        try:
            import langchain_community
            diagnostics["langchain_community_version"] = langchain_community.__version__
        except:
            diagnostics["langchain_community_version"] = "No disponible"
            
        return diagnostics

    def fix_chromadb_config(self):
        """Intenta reparar la configuraciÃ³n corrupta de ChromaDB."""
        try:
            import shutil
            import sqlite3
            
            # Backup del directorio actual
            if os.path.exists(CHROMA_PATH):
                backup_path = f"{CHROMA_PATH}_backup_{int(datetime.now().timestamp())}"
                logger.info(f"Creando backup en: {backup_path}")
                shutil.copytree(CHROMA_PATH, backup_path)
                
                # Intentar limpiar la base de datos
                db_path = os.path.join(CHROMA_PATH, "chroma.sqlite3")
                if os.path.exists(db_path):
                    try:
                        conn = sqlite3.connect(db_path)
                        cursor = conn.cursor()
                        
                        # Verificar la tabla de configuraciones
                        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='collections';")
                        if cursor.fetchone():
                            # Limpiar configuraciones corruptas
                            cursor.execute("DELETE FROM collections WHERE configuration NOT LIKE '%_type%';")
                            conn.commit()
                            logger.info("Configuraciones corruptas limpiadas")
                        
                        conn.close()
                        return True
                        
                    except Exception as db_error:
                        logger.error(f"Error reparando BD: {db_error}")
                        conn.close()
                
                # Si no se puede reparar, eliminar completamente
                logger.info("Eliminando directorio ChromaDB corrupto")
                shutil.rmtree(CHROMA_PATH)
                return True
                
        except Exception as e:
            logger.error(f"Error en fix_chromadb_config: {e}")
            
        return False

    def initialize_system(self):
        """Inicializa el sistema de IA completo de forma sÃ­ncrona."""
        if self.is_initialized:
            logger.info("Sistema ya inicializado")
            return True
            
        try:
            logger.info("ðŸš€ Iniciando sistema de IA...")
            
            # Verificar dependencias de ChromaDB
            logger.info("ðŸ“‹ Verificando dependencias...")
            chromadb_available = self.check_chromadb_dependencies()
            logger.info(f"ChromaDB dependencies available: {chromadb_available}")
            
            # Procesar PDFs
            logger.info("ðŸ“š Procesando archivos PDF...")
            self.documentos, self.fragmentos = process_pdf_files_optimized(PDFS_DIR, CACHE_DIR)
            
            if not self.fragmentos:
                logger.warning("No se encontraron fragmentos en cache, cargando PDFs directamente...")
                pdf_files = glob(os.path.join(PDFS_DIR, "*.pdf"))
                logger.info(f"Archivos PDF encontrados: {len(pdf_files)}")
                
                if pdf_files:
                    for i, pdf_path in enumerate(pdf_files):
                        try:
                            logger.info(f"ðŸ“„ Cargando PDF {i+1}/{len(pdf_files)}: {os.path.basename(pdf_path)}")
                            loader = PyPDFLoader(pdf_path)
                            self.documentos.extend(loader.load())
                        except Exception as e:
                            logger.error(f"Error cargando {pdf_path}: {e}")
                    
                    if self.documentos:
                        logger.info(f"âœ… Documentos cargados: {len(self.documentos)}")
                        logger.info("ðŸ”§ Fragmentando documentos...")
                        splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
                        self.fragmentos = splitter.split_documents(self.documentos)
                        logger.info(f"âœ… Fragmentos generados: {len(self.fragmentos)}")
            else:
                logger.info(f"âœ… Fragmentos cargados desde cache: {len(self.fragmentos)}")
            
            # Inicializar embeddings y LLM
            logger.info("ðŸ§  Inicializando modelo de lenguaje...")
            try:
                embeddings = OllamaEmbeddings(model="nomic-embed-text")
                self.llm = self.get_or_create_llm(DEFAULT_MODEL)
                self.current_model = DEFAULT_MODEL
                logger.info(f"âœ… Embeddings y LLM inicializados correctamente con modelo: {DEFAULT_MODEL}")
            except Exception as e:
                logger.error(f"âŒ Error inicializando Ollama: {e}")
                raise
            
            # Configurar base de datos vectorial con optimizaciÃ³n
            if chromadb_available and self.fragmentos:
                success = self.setup_vector_database()
                if not success:
                    logger.warning("âš ï¸ FallÃ³ configuraciÃ³n vectorial, usando modo bÃ¡sico")
            else:
                if not chromadb_available:
                    logger.warning("âš ï¸ ChromaDB no disponible")
                elif not self.fragmentos:
                    logger.warning("âš ï¸ No hay fragmentos para procesar")
            
            # Configurar cadena de recuperaciÃ³n
            if self.vector_store is not None:
                logger.info("âš™ï¸ Configurando RetrievalQA...")
                retriever = self.vector_store.as_retriever(search_kwargs={"k": 3})
                
                self.cadena = RetrievalQA.from_chain_type(
                    llm=self.llm,
                    chain_type="stuff",
                    retriever=retriever,
                    return_source_documents=True,
                    chain_type_kwargs={"prompt": PROMPT_QA_SIMPLE}
                )
                
                logger.info(f"âœ… RetrievalQA configurado con {'ChromaDB' if self.using_chroma else 'FAISS'}")
            else:
                logger.warning("âš ï¸ No se pudo configurar vector store - funcionando sin RAG")
            
            # Crear memoria para conversaciÃ³n
            self.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
            
            self.is_initialized = True
            logger.info(f"ðŸŽ‰ Sistema IA LISTO: ChromaDB={self.using_chroma}, VectorDB={self.using_vector_db}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error crÃ­tico en inicializaciÃ³n: {e}")
            logger.error(f"Traceback completo:\n{traceback.format_exc()}")
            self.setup_fallback()
            return False

    def setup_fallback(self):
        """Configura el sistema en modo fallback."""
        try:
            logger.info("Configurando sistema en modo fallback...")
            self.llm = self.get_or_create_llm(DEFAULT_MODEL)
            self.current_model = DEFAULT_MODEL
            self.cadena = load_qa_chain(self.llm, chain_type="stuff")
            self.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
            self.using_vector_db = False
            logger.info("Sistema fallback configurado")
        except Exception as e:
            logger.error(f"Error en fallback: {e}")
    
    def detect_question_type(self, question_text):
        """Detecta el tipo de pregunta basado en patrones."""
        patrones_tipo_pregunta = {
            "saludo": [r"^(hola|buenos dÃ­as|buenas tardes|buenas noches|saludos|hey|quÃ© tal|cÃ³mo estÃ¡s)"],
            "despedida": [r"^(adiÃ³s|chao|hasta luego|nos vemos|gracias por tu ayuda|gracias|hasta pronto)"],
            "definicion": [r"(quÃ©|que) (es|son|significa|significan)", r"define|definir|definiciÃ³n", r"\b(explica|explique|explicar)\b"],
            "ejemplo": [r"(dame|da|muestra|pon) (un|unos|algunos)? ejemplos?", r"ejemplos? de"],
            "comparacion": [r"(compara|comparaciÃ³n|diferencias?) (entre|de)", r"(quÃ©|que|cuÃ¡l|cual) es (mejor|peor)"],
            "resumen": [r"(resume|resumen|sÃ­ntesis|sintetiza)", r"(en pocas palabras|brevemente)"],
            "aclaracion": [r"(puedes|podrÃ­as) (explicar|aclarar) (mejor|de nuevo)", r"no (entiendo|comprendo)"],
            "seguimiento": [r"(y|pero) (quÃ©|que|cÃ³mo|como)", r"(puedes|podrÃ­as) (ampliar|expandir)"],
            "syllabus": [r"(syllabus|programa|temario|contenido)", r"(coordinador|profesor|docente)"]
        }
        
        for tipo, patrones in patrones_tipo_pregunta.items():
            for patron in patrones:
                if re.search(patron, question_text.lower()):
                    return tipo
        
        return None
    
    def get_template_by_type(self, tipo_pregunta):
        """Retorna la plantilla apropiada segÃºn el tipo de pregunta."""
        template_map = {
            "saludo": plantilla_saludo,
            "despedida": plantilla_despedida,
            "definicion": plantilla_definicion,
            "ejemplo": plantilla_ejemplo,
            "comparacion": plantilla_comparacion,
            "resumen": plantilla_resumen,
            "aclaracion": plantilla_aclaracion,
            "seguimiento": plantilla_seguimiento,
            "syllabus": plantilla_especifica
        }
        
        return template_map.get(tipo_pregunta)
    
    def retrieve_relevant_documents(self, query):
        """Recupera documentos relevantes para la consulta."""
        try:
            if self.vector_store is not None:
                retriever = self.vector_store.as_retriever(
                    search_type="mmr",
                    search_kwargs={"k": 8, "lambda_mult": 0.5}
                )
                # Usar invoke en lugar de get_relevant_documents (deprecado)
                return retriever.invoke(query)
            elif self.fragmentos:
                import random
                return random.sample(self.fragmentos, min(5, len(self.fragmentos)))
        except Exception as e:
            logger.error(f"Error recuperando documentos: {e}")
        
        return []
    
    def generate_search_query(self, question, conversation_history):
        """Genera una consulta de bÃºsqueda optimizada."""
        try:
            # Usar RunnableSequence en lugar de LLMChain (deprecado)
            cadena_reescritura = plantilla_reescritura_pregunta | self.llm
            
            result = cadena_reescritura.invoke({
                "historial_chat": conversation_history, 
                "pregunta": question
            })
            
            # El resultado directo del pipeline es el texto
            return result.strip() if isinstance(result, str) else str(result).strip()
        except Exception as e:
            logger.error(f"Error generando consulta de bÃºsqueda: {e}")
            return question
    
    def process_question(self, pregunta_obj):
        """Procesa la pregunta principal usando el sistema completo."""
        question_text = pregunta_obj.texto
        
        # Manejar cambio de modelo si se especifica
        if hasattr(pregunta_obj, 'modelo') and pregunta_obj.modelo:
            if pregunta_obj.modelo != self.current_model:
                logger.info(f"Cambiando modelo para esta consulta: {pregunta_obj.modelo}")
                self.switch_model(pregunta_obj.modelo)
        
        # Construir historial de conversaciÃ³n
        conversation_history = ""
        if pregunta_obj.history and isinstance(pregunta_obj.history, list):
            history_limit = min(10, len(pregunta_obj.history))
            recent_history = pregunta_obj.history[-history_limit:]
            
            for msg in recent_history:
                if msg.get('sender') == 'user':
                    conversation_history += f"Usuario: {msg.get('text','')}\n"
                elif msg.get('sender') == 'bot':
                    conversation_history += f"Bot: {msg.get('text','')}\n"
        
        # Analizar tipo y complejidad
        tipo_pregunta = self.detect_question_type(question_text)
        tipo_respuesta = analyze_question_complexity(question_text)
        
        logger.info(f"Tipo pregunta: {tipo_pregunta}, Tipo respuesta: {tipo_respuesta}")
        
        # Generar consulta optimizada
        search_query = self.generate_search_query(question_text, conversation_history)
        
        # Recuperar documentos relevantes
        documentos_relevantes = self.retrieve_relevant_documents(search_query)
        contexto_documentos = "\n".join([doc.page_content for doc in documentos_relevantes])
        
        # Seleccionar plantilla y generar respuesta
        plantilla_seleccionada = self.get_template_by_type(tipo_pregunta)
        usar_retrieval_qa = plantilla_seleccionada is None
        
        try:
            if usar_retrieval_qa and self.cadena and self.using_vector_db:
                result = self.cadena.invoke({"query": question_text})
                respuesta = result["result"]
                logger.info("Respuesta generada con RetrievalQA")
            
            elif plantilla_seleccionada:
                respuesta = self._generate_with_template(
                    plantilla_seleccionada, question_text, 
                    conversation_history, contexto_documentos
                )
                logger.info("Respuesta generada con plantilla personalizada")
            
            else:
                respuesta = self._generate_fallback_response(
                    question_text, conversation_history, contexto_documentos
                )
                logger.info("Respuesta generada con fallback")
            
            # Limpiar respuesta
            respuesta_limpia = clean_llm_response(respuesta)
            respuesta_limpia = self._clean_response_artifacts(respuesta_limpia)
            
            return respuesta_limpia.strip()
            
        except Exception as e:
            logger.error(f"Error procesando pregunta: {e}")
            return "Lo siento, no puedo procesar tu pregunta en este momento."
    
    def _generate_with_template(self, template, question, conversation, context_docs):
        """Genera respuesta usando plantilla especÃ­fica."""
        if template in [plantilla_saludo, plantilla_despedida]:
            return self.llm.invoke(template.format(contexto=contexto_base))
        
        template_modificado = (
            "{contexto}\n\n"
            "INFORMACIÃ“N RELEVANTE DE DOCUMENTOS:\n"
            "{contexto_documentos}\n\n"
            "{instrucciones_especificas}\n\n"
            "ConversaciÃ³n previa:\n{conversacion}\n\n"
            "Pregunta: {pregunta}\n\n"
            "Respuesta basada en documentos y contexto:"
        )
        
        # Extraer instrucciones especÃ­ficas
        template_str = template.template
        instrucciones_match = re.search(r'Instrucciones especÃ­ficas: ([^\\n]+(?:\\n[^\\n]+)*?)\\n\\n', template_str)
        instrucciones = instrucciones_match.group(1) if instrucciones_match else "Responde de manera clara y educativa."
        
        prompt_formateado = template_modificado.format(
            contexto=contexto_base,
            contexto_documentos=context_docs or "No se encontraron documentos especÃ­ficamente relevantes.",
            instrucciones_especificas=instrucciones,
            conversacion=conversation,
            pregunta=question
        )
        
        return self.llm.invoke(prompt_formateado)
    
    def _generate_fallback_response(self, question, conversation, context_docs):
        """Genera respuesta usando el mÃ©todo fallback general."""
        prompt_fallback = f"""
        {contexto_base}
        
        INFORMACIÃ“N RELEVANTE DE DOCUMENTOS:
        {context_docs or "No se encontraron documentos especÃ­ficamente relevantes."}
        
        CONVERSACIÃ“N PREVIA:
        {conversation}
        
        PREGUNTA DEL ESTUDIANTE: {question}
        
        Responde de manera educativa y clara, usando la informaciÃ³n de los documentos cuando sea relevante:
        """
        
        return self.llm.invoke(prompt_fallback)
    
    def _clean_response_artifacts(self, response):
        """Limpia artefactos comunes en las respuestas del LLM."""
        # Eliminar frases introductorias genÃ©ricas
        response = re.sub(r'^Como (un )?asistente educativo( conversacional)?,?\s*', '', response, flags=re.IGNORECASE)
        response = re.sub(r'^Basado en (el syllabus|los documentos|la informaciÃ³n)( del curso)?,?\s*', '', response, flags=re.IGNORECASE)
        response = re.sub(r'^Â¡Claro! AquÃ­ tienes un resumen:\s*', '', response, flags=re.IGNORECASE)
        
        # Normalizar saltos de lÃ­nea
        response = re.sub(r'\n{3,}', '\n\n', response)
        
        return response

    def generate_dynamic_suggestions(self, conversation_history):
        """Genera sugerencias dinÃ¡micas basadas en el historial de conversaciÃ³n."""
        try:
            if not self.llm or not conversation_history:
                return self._get_fallback_suggestions()
            
            # Extraer la Ãºltima respuesta del bot (limitada para velocidad)
            ultima_respuesta_bot = ""
            
            # Buscar solo la Ãºltima respuesta del bot (mÃ¡s rÃ¡pido)
            for mensaje in reversed(conversation_history):
                if isinstance(mensaje, dict) and mensaje.get('sender') == 'bot':
                    ultima_respuesta_bot = mensaje.get('text', '')
                    break
            
            if not ultima_respuesta_bot or len(ultima_respuesta_bot) < 20:
                logger.info("No se encontrÃ³ respuesta vÃ¡lida del bot, usando sugerencias por defecto")
                return self._get_fallback_suggestions()
            
            # Limitar el tamaÃ±o de la respuesta para velocidad (mÃ¡ximo 300 caracteres)
            if len(ultima_respuesta_bot) > 300:
                ultima_respuesta_bot = ultima_respuesta_bot[:300] + "..."
            
            # Usar plantilla simplificada para generar sugerencias
            from templates import plantilla_sugerencias_dinamicas
            
            prompt_formateado = plantilla_sugerencias_dinamicas.format(
                ultima_respuesta=ultima_respuesta_bot,
                contexto_conversacion="IA y Machine Learning"  # Contexto fijo para velocidad
            )
            
            logger.info("Generando sugerencias dinÃ¡micas rÃ¡pidas")
            respuesta_llm = self.llm.invoke(prompt_formateado)
            
            # Procesar la respuesta para extraer las sugerencias
            sugerencias = self._parse_suggestions_from_response_fast(respuesta_llm)
            
            if len(sugerencias) >= 2:  # Reducir requisito a 2 sugerencias mÃ­nimo
                logger.info(f"Sugerencias dinÃ¡micas generadas exitosamente: {len(sugerencias)}")
                # Asegurar que tenemos exactamente 3 sugerencias
                while len(sugerencias) < 3:
                    sugerencias.extend(self._get_contextual_fallback_suggestions(ultima_respuesta_bot))
                return sugerencias[:3]
            else:
                logger.info("LLM no generÃ³ suficientes sugerencias, usando fallback contextual")
                return self._get_contextual_fallback_suggestions(ultima_respuesta_bot)
                
        except Exception as e:
            logger.error(f"Error generando sugerencias dinÃ¡micas: {e}")
            return self._get_fallback_suggestions()
    
    def _parse_suggestions_from_response_fast(self, respuesta_llm):
        """VersiÃ³n optimizada para extraer sugerencias mÃ¡s rÃ¡pido."""
        try:
            # Limpiar la respuesta
            respuesta_limpia = respuesta_llm.strip()
            
            # Dividir por lÃ­neas y procesar mÃ¡s eficientemente
            lineas = [linea.strip() for linea in respuesta_limpia.split('\n') if linea.strip()]
            
            sugerencias = []
            for linea in lineas[:6]:  # Solo revisar las primeras 6 lÃ­neas para velocidad
                # Limpiar numeraciÃ³n bÃ¡sica
                linea_limpia = re.sub(r'^\d+[\.\)\-\s]*', '', linea).strip()
                linea_limpia = re.sub(r'^[\-\*\â€¢\s]+', '', linea_limpia).strip()
                
                # Verificar que sea una pregunta vÃ¡lida y corta
                if (linea_limpia and 
                    (linea_limpia.startswith('Â¿') or linea_limpia.lower().startswith(('quÃ©', 'cÃ³mo', 'por quÃ©', 'cuÃ¡l', 'cuÃ¡les'))) and
                    len(linea_limpia) > 8 and len(linea_limpia.split()) <= 12):  # Aumentar lÃ­mite ligeramente
                    
                    # Asegurar que termine con signo de interrogaciÃ³n
                    if not linea_limpia.endswith('?'):
                        linea_limpia += '?'
                    
                    sugerencias.append(linea_limpia)
                    
                    # Salir temprano si ya tenemos 3 sugerencias
                    if len(sugerencias) >= 3:
                        break
            
            return sugerencias
            
        except Exception as e:
            logger.error(f"Error procesando sugerencias del LLM: {e}")
            return []
    
    def _get_contextual_fallback_suggestions(self, ultima_respuesta):
        """Genera sugerencias contextuales basadas en palabras clave de la Ãºltima respuesta."""
        try:
            respuesta_lower = ultima_respuesta.lower()
            
            # Detectar palabras clave y generar sugerencias especÃ­ficas
            if any(keyword in respuesta_lower for keyword in ['machine learning', 'aprendizaje automÃ¡tico', 'ml']):
                return [
                    "Â¿QuÃ© tipos de ML existen?",
                    "Â¿CÃ³mo funciona el aprendizaje supervisado?",
                    "Â¿CuÃ¡les son las aplicaciones principales?"
                ]
            elif any(keyword in respuesta_lower for keyword in ['algoritmo', 'algoritmos']):
                return [
                    "Â¿QuÃ© algoritmos son mÃ¡s eficientes?",
                    "Â¿CÃ³mo se evalÃºa un algoritmo?",
                    "Â¿CuÃ¡l es mejor para clasificaciÃ³n?"
                ]
            elif any(keyword in respuesta_lower for keyword in ['red neural', 'redes neuronales', 'neural']):
                return [
                    "Â¿CÃ³mo funciona el backpropagation?",
                    "Â¿QuÃ© son las capas ocultas?",
                    "Â¿CuÃ¡ndo usar redes profundas?"
                ]
            elif any(keyword in respuesta_lower for keyword in ['datos', 'dataset', 'entrenamiento']):
                return [
                    "Â¿CÃ³mo preparar los datos?",
                    "Â¿QuÃ© es el overfitting?",
                    "Â¿CuÃ¡ntos datos necesito?"
                ]
            elif any(keyword in respuesta_lower for keyword in ['ia', 'inteligencia artificial', 'ai']):
                return [
                    "Â¿QuÃ© es la IA simbÃ³lica?",
                    "Â¿CÃ³mo funciona el procesamiento de lenguaje?",
                    "Â¿CuÃ¡les son las aplicaciones actuales?"
                ]
            else:
                return self._get_fallback_suggestions()
                
        except Exception as e:
            logger.error(f"Error en sugerencias contextuales: {e}")
            return self._get_fallback_suggestions()
    
    def _get_fallback_suggestions(self):
        """Retorna sugerencias por defecto cuando no se pueden generar dinÃ¡micamente."""
        return [
            "Â¿QuÃ© es la inteligencia artificial?",
            "Â¿CÃ³mo funciona el machine learning?",
            "Â¿CuÃ¡les son los algoritmos principales?"
        ]

# NO crear instancia global automÃ¡ticamente
# ai_system = AISystem()
ai_system = None

def get_ai_system():
    """FunciÃ³n para obtener la instancia del sistema IA"""
    global ai_system
    if ai_system is None:
        ai_system = AISystem()
    return ai_system
