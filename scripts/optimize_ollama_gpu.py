#!/usr/bin/env python3
"""
T2.1: OptimizaciÃ³n GPU para Ollama con RTX 3060
Configurar Ollama para mÃ¡ximo rendimiento en GPU
"""

import requests
import json
import subprocess
import os
from datetime import datetime

def check_ollama_status():
    """Verificar estado actual de Ollama"""
    try:
        response = requests.get('http://localhost:11434/api/version', timeout=5)
        if response.status_code == 200:
            print("âœ… Ollama funcionando correctamente")
            return True
        else:
            print("âŒ Ollama no responde")
            return False
    except Exception as e:
        print(f"âŒ Error conectando a Ollama: {e}")
        return False

def configure_ollama_gpu():
    """Configurar variables de entorno para optimizar GPU"""
    print("ğŸ”§ Configurando Ollama para RTX 3060...")
    
    # ConfiguraciÃ³n optimizada para RTX 3060 12GB
    gpu_config = {
        'OLLAMA_GPU_MEMORY_FRACTION': '0.85',  # 85% de 12GB = 10.2GB
        'OLLAMA_NUM_PARALLEL': '2',            # 2 requests simultÃ¡neos
        'OLLAMA_MAX_LOADED_MODELS': '1',       # 1 modelo en VRAM
        'OLLAMA_NUM_THREAD': '8',              # 8 threads CPU
        'OLLAMA_KEEP_ALIVE': '10m',            # Mantener modelo 10 min
        'OLLAMA_HOST': '0.0.0.0:11434',        # Host binding
        'CUDA_VISIBLE_DEVICES': '0'            # Solo GPU 0
    }
    
    # Crear script de configuraciÃ³n
    script_content = "@echo off\n"
    script_content += "echo Configurando Ollama para RTX 3060...\n"
    
    for key, value in gpu_config.items():
        script_content += f"set {key}={value}\n"
        os.environ[key] = value
        print(f"   {key}={value}")
    
    script_content += "echo Reiniciando Ollama con nueva configuracion...\n"
    script_content += "taskkill /F /IM ollama.exe >nul 2>&1\n"
    script_content += "timeout /t 2 >nul\n"
    script_content += "start \"\" ollama serve\n"
    script_content += "echo Ollama optimizado iniciado\n"
    
    # Guardar script
    with open('optimize_ollama_gpu.bat', 'w') as f:
        f.write(script_content)
    
    print("âœ… Script de optimizaciÃ³n creado: optimize_ollama_gpu.bat")
    return gpu_config

def test_gpu_utilization():
    """Test de utilizaciÃ³n GPU"""
    print("\nğŸ§ª TESTING GPU UTILIZATION...")
    
    test_prompts = [
        "Â¿QuÃ© es la inteligencia artificial?",
        "Explica el aprendizaje automÃ¡tico",
        "Define las redes neuronales"
    ]
    
    results = []
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nğŸ“ Test {i}/3: {prompt[:30]}...")
        
        start_time = datetime.now()
        
        try:
            response = requests.post('http://localhost:11434/api/generate', 
                json={
                    'model': 'llama3',
                    'prompt': prompt,
                    'stream': False
                },
                timeout=30
            )
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            if response.status_code == 200:
                result = response.json()
                response_text = result.get('response', '')
                
                results.append({
                    'prompt': prompt,
                    'duration': duration,
                    'response_length': len(response_text),
                    'success': True
                })
                
                print(f"   âœ… Completado en {duration:.2f}s")
                print(f"   ğŸ“Š Respuesta: {len(response_text)} caracteres")
            else:
                print(f"   âŒ Error HTTP: {response.status_code}")
                results.append({
                    'prompt': prompt,
                    'duration': duration,
                    'success': False,
                    'error': f"HTTP {response.status_code}"
                })
                
        except Exception as e:
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            print(f"   âŒ Error: {e}")
            results.append({
                'prompt': prompt,
                'duration': duration,
                'success': False,
                'error': str(e)
            })
    
    return results

def analyze_performance(results):
    """Analizar resultados de performance"""
    print("\nğŸ“Š ANÃLISIS DE RENDIMIENTO")
    print("="*50)
    
    successful_tests = [r for r in results if r['success']]
    
    if successful_tests:
        durations = [r['duration'] for r in successful_tests]
        avg_duration = sum(durations) / len(durations)
        min_duration = min(durations)
        max_duration = max(durations)
        
        print(f"âœ… Tests exitosos: {len(successful_tests)}/{len(results)}")
        print(f"â±ï¸ Tiempo promedio: {avg_duration:.2f}s")
        print(f"ğŸƒ Tiempo mÃ­nimo: {min_duration:.2f}s")
        print(f"ğŸŒ Tiempo mÃ¡ximo: {max_duration:.2f}s")
        
        # Comparar con baseline
        baseline = 15.72  # Tiempo promedio anterior
        improvement = ((baseline - avg_duration) / baseline) * 100
        
        if improvement > 0:
            print(f"ğŸš€ Mejora: {improvement:.1f}% mÃ¡s rÃ¡pido")
        else:
            print(f"âš ï¸ DegradaciÃ³n: {abs(improvement):.1f}% mÃ¡s lento")
            
        return {
            'avg_duration': avg_duration,
            'improvement_percent': improvement,
            'success_rate': len(successful_tests) / len(results) * 100
        }
    else:
        print("âŒ No hay tests exitosos para analizar")
        return None

def generate_optimization_report(config, performance):
    """Generar reporte de optimizaciÃ³n"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    report = f"""
# REPORTE OPTIMIZACIÃ“N OLLAMA GPU - RTX 3060
Generado: {timestamp}

## CONFIGURACIÃ“N APLICADA:
"""
    
    for key, value in config.items():
        report += f"- {key}: {value}\n"
    
    if performance:
        report += f"""
## RESULTADOS DE RENDIMIENTO:
- Tiempo promedio: {performance['avg_duration']:.2f}s
- Mejora respecto a baseline: {performance['improvement_percent']:.1f}%
- Tasa de Ã©xito: {performance['success_rate']:.1f}%

## RECOMENDACIONES:
"""
        if performance['improvement_percent'] > 10:
            report += "âœ… OptimizaciÃ³n exitosa - mantener configuraciÃ³n\n"
        elif performance['improvement_percent'] > 0:
            report += "âš ï¸ Mejora marginal - considerar ajustes adicionales\n"
        else:
            report += "âŒ Sin mejora - revisar configuraciÃ³n\n"
    
    with open('optimization_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ“„ Reporte guardado en: optimization_report.md")

def main():
    print("ğŸš€ OPTIMIZACIÃ“N OLLAMA GPU - RTX 3060")
    print("="*50)
    
    # Verificar Ollama
    if not check_ollama_status():
        print("âŒ Ollama no estÃ¡ funcionando. Iniciarlo primero.")
        return
    
    # Configurar GPU
    config = configure_ollama_gpu()
    
    print(f"\nâš ï¸ IMPORTANTE:")
    print("1. Ejecutar: .\\optimize_ollama_gpu.bat")
    print("2. Esperar 10 segundos para que Ollama reinicie")
    print("3. Ejecutar este script nuevamente para testing")
    
    # Confirmar si continuar con testing
    print(f"\nÂ¿Ollama ya fue reiniciado con la nueva configuraciÃ³n? (s/n): ", end="")
    
    # Para demo, asumimos que sÃ­
    response = "s"  # input().strip().lower()
    
    if response == 's':
        print("\nğŸ§ª Iniciando tests de rendimiento...")
        results = test_gpu_utilization()
        performance = analyze_performance(results)
        generate_optimization_report(config, performance)
        
        print(f"\nğŸ¯ SIGUIENTE PASO:")
        print("T2.2: OptimizaciÃ³n Workers Celery (30min)")
    else:
        print("\nâ¸ï¸ Testing pospuesto. Ejecutar script despuÃ©s del reinicio.")

if __name__ == "__main__":
    main()
