#!/usr/bin/env python3
"""
T2.3: Cach√© Redis avanzado para respuestas IA
Implementar cach√© inteligente con TTL, invalidaci√≥n y compresi√≥n
"""

import redis
import json
import hashlib
import gzip
import base64
from datetime import datetime, timedelta
import requests
import time

class AdvancedRedisCache:
    """Cach√© Redis avanzado con compresi√≥n y TTL inteligente"""
    
    def __init__(self, host='localhost', port=6379, db=0):
        self.redis_client = redis.Redis(host=host, port=port, db=db, decode_responses=True)
        self.compression_threshold = 500  # Comprimir si > 500 chars
        
    def _generate_cache_key(self, prompt, model='llama3'):
        """Generar clave √∫nica basada en prompt y modelo"""
        # Normalizar prompt (lowercase, trim)
        normalized = prompt.lower().strip()
        
        # Hash del prompt + modelo
        key_data = f"{normalized}|{model}"
        hash_key = hashlib.md5(key_data.encode()).hexdigest()
        
        return f"chat_cache:{model}:{hash_key}"
    
    def _compress_data(self, data):
        """Comprimir datos si exceden threshold"""
        json_str = json.dumps(data, ensure_ascii=False)
        
        if len(json_str) > self.compression_threshold:
            # Comprimir con gzip
            compressed = gzip.compress(json_str.encode('utf-8'))
            encoded = base64.b64encode(compressed).decode('ascii')
            return {
                'compressed': True,
                'data': encoded
            }
        else:
            return {
                'compressed': False,
                'data': data
            }
    
    def _decompress_data(self, cached_data):
        """Descomprimir datos si est√°n comprimidos"""
        if cached_data.get('compressed', False):
            # Descomprimir
            encoded_data = cached_data['data']
            compressed = base64.b64decode(encoded_data.encode('ascii'))
            json_str = gzip.decompress(compressed).decode('utf-8')
            return json.loads(json_str)
        else:
            return cached_data['data']
    
    def get_cached_response(self, prompt, model='llama3'):
        """Obtener respuesta del cach√©"""
        try:
            cache_key = self._generate_cache_key(prompt, model)
            cached = self.redis_client.get(cache_key)
            
            if cached:
                cached_data = json.loads(cached)
                response = self._decompress_data(cached_data)
                
                # Agregar metadata de cach√©
                response['cache_hit'] = True
                response['cached_at'] = cached_data.get('cached_at')
                
                return response
                
            return None
            
        except Exception as e:
            print(f"‚ùå Error obteniendo cach√©: {e}")
            return None
    
    def cache_response(self, prompt, response, model='llama3', ttl_hours=24):
        """Guardar respuesta en cach√©"""
        try:
            cache_key = self._generate_cache_key(prompt, model)
            
            # Preparar datos para cach√©
            cache_data = {
                'response': response,
                'model': model,
                'cached_at': datetime.now().isoformat(),
                'prompt_hash': hashlib.md5(prompt.encode()).hexdigest()[:8]
            }
            
            # Comprimir si es necesario
            compressed_data = self._compress_data(cache_data)
            
            # Agregar metadata de compresi√≥n
            final_data = {
                **compressed_data,
                'cached_at': cache_data['cached_at']
            }
            
            # Guardar con TTL
            ttl_seconds = ttl_hours * 3600
            self.redis_client.setex(
                cache_key, 
                ttl_seconds, 
                json.dumps(final_data, ensure_ascii=False)
            )
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error guardando cach√©: {e}")
            return False
    
    def get_cache_stats(self):
        """Obtener estad√≠sticas del cach√©"""
        try:
            # Contar claves de cach√©
            cache_keys = self.redis_client.keys('chat_cache:*')
            
            stats = {
                'total_cached_responses': len(cache_keys),
                'memory_usage_mb': 0,
                'oldest_cache': None,
                'newest_cache': None,
                'compression_ratio': 0
            }
            
            if cache_keys:
                # Calcular estad√≠sticas detalladas
                cache_dates = []
                total_original = 0
                total_compressed = 0
                
                for key in cache_keys[:100]:  # Sample de 100 para performance
                    try:
                        cached_str = self.redis_client.get(key)
                        if cached_str:
                            cached_data = json.loads(cached_str)
                            
                            # Fecha de cach√©
                            cached_at = cached_data.get('cached_at')
                            if cached_at:
                                cache_dates.append(cached_at)
                            
                            # Estad√≠sticas de compresi√≥n
                            if cached_data.get('compressed', False):
                                # Estimar tama√±o original vs comprimido
                                compressed_size = len(cached_data['data'])
                                estimated_original = compressed_size * 3  # Estimaci√≥n
                                total_original += estimated_original
                                total_compressed += compressed_size
                            else:
                                size = len(json.dumps(cached_data['data']))
                                total_original += size
                                total_compressed += size
                                
                    except:
                        continue
                
                if cache_dates:
                    cache_dates.sort()
                    stats['oldest_cache'] = cache_dates[0]
                    stats['newest_cache'] = cache_dates[-1]
                
                if total_original > 0:
                    stats['compression_ratio'] = (1 - total_compressed / total_original) * 100
            
            # Memoria utilizada (aproximada)
            info = self.redis_client.info('memory')
            stats['memory_usage_mb'] = round(info.get('used_memory', 0) / 1024 / 1024, 2)
            
            return stats
            
        except Exception as e:
            print(f"‚ùå Error obteniendo estad√≠sticas: {e}")
            return {}
    
    def clear_expired_cache(self):
        """Limpiar cach√© expirado manualmente"""
        try:
            cleared = 0
            cache_keys = self.redis_client.keys('chat_cache:*')
            
            for key in cache_keys:
                # Redis maneja TTL autom√°ticamente, pero verificamos
                ttl = self.redis_client.ttl(key)
                if ttl == -2:  # Clave expirada
                    self.redis_client.delete(key)
                    cleared += 1
            
            return cleared
            
        except Exception as e:
            print(f"‚ùå Error limpiando cach√©: {e}")
            return 0

def test_cache_performance():
    """Test de rendimiento del cach√©"""
    print("üß™ TESTING CACH√â REDIS AVANZADO")
    print("="*50)
    
    cache = AdvancedRedisCache()
    
    # Test prompts
    test_prompts = [
        "¬øQu√© es la inteligencia artificial?",
        "Explica el machine learning",
        "¬øC√≥mo funcionan las redes neuronales?",
        "Define deep learning",
        "¬øQu√© son los algoritmos gen√©ticos?"
    ]
    
    print("üìù Ejecutando requests SIN cach√©...")
    
    # Primera ronda - sin cach√©
    no_cache_times = []
    for i, prompt in enumerate(test_prompts, 1):
        print(f"   Request {i}/5: {prompt[:30]}...")
        
        start_time = time.time()
        
        try:
            response = requests.post('http://localhost:8000/chat/async', 
                json={
                    'texto': prompt,
                    'modelo': 'llama3'
                },
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                task_id = data.get('task_id')
                
                # Esperar resultado
                max_wait = 60
                wait_time = 0
                completed = False
                result_data = None
                
                while wait_time < max_wait and not completed:
                    time.sleep(2)
                    wait_time += 2
                    
                    result_response = requests.get(f'http://localhost:8000/chat/status/{task_id}')
                    if result_response.status_code == 200:
                        result_data = result_response.json()
                        if result_data.get('status') == 'completed':
                            completed = True
                            break
                
                duration = time.time() - start_time
                no_cache_times.append(duration)
                
                if completed and result_data:
                    # Guardar en cach√©
                    ai_response = result_data.get('result', {}).get('response', '')
                    cache.cache_response(prompt, ai_response)
                    print(f"      ‚úÖ {duration:.2f}s (guardado en cach√©)")
                else:
                    print(f"      ‚è∞ {duration:.2f}s (timeout)")
            else:
                print(f"      ‚ùå HTTP {response.status_code}")
                
        except Exception as e:
            print(f"      ‚ùå Error: {e}")
    
    print(f"\nüìù Ejecutando mismos requests CON cach√©...")
    
    # Segunda ronda - con cach√©
    cache_times = []
    cache_hits = 0
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"   Request {i}/5: {prompt[:30]}...")
        
        start_time = time.time()
        
        # Intentar obtener del cach√©
        cached_response = cache.get_cached_response(prompt)
        duration = time.time() - start_time
        
        if cached_response:
            cache_hits += 1
            cache_times.append(duration)
            print(f"      üöÄ {duration:.4f}s (CACHE HIT)")
        else:
            print(f"      ‚ùå Cache miss")
    
    return no_cache_times, cache_times, cache_hits

def analyze_cache_performance(no_cache_times, cache_times, cache_hits):
    """Analizar resultados del cach√©"""
    print(f"\nüìä AN√ÅLISIS DE RENDIMIENTO CACH√â")
    print("="*50)
    
    if no_cache_times and cache_times:
        avg_no_cache = sum(no_cache_times) / len(no_cache_times)
        avg_cache = sum(cache_times) / len(cache_times)
        
        speedup = avg_no_cache / avg_cache if avg_cache > 0 else 0
        time_saved = avg_no_cache - avg_cache
        
        print(f"‚è±Ô∏è Tiempo promedio SIN cach√©: {avg_no_cache:.2f}s")
        print(f"üöÄ Tiempo promedio CON cach√©: {avg_cache:.4f}s")
        print(f"üìà Speedup: {speedup:.0f}x m√°s r√°pido")
        print(f"üíæ Tiempo ahorrado: {time_saved:.2f}s por request")
        print(f"üéØ Cache hit rate: {cache_hits}/{len(cache_times)} ({cache_hits/len(cache_times)*100:.1f}%)")
        
        return {
            'speedup': speedup,
            'time_saved': time_saved,
            'hit_rate': cache_hits/len(cache_times)*100
        }
    
    return None

def main():
    print("üöÄ OPTIMIZACI√ìN CACH√â REDIS AVANZADO")
    print("="*50)
    
    # Verificar conexi√≥n Redis
    try:
        cache = AdvancedRedisCache()
        cache.redis_client.ping()
        print("‚úÖ Redis conectado correctamente")
    except Exception as e:
        print(f"‚ùå Error conectando a Redis: {e}")
        print("   Aseg√∫rate de que Docker est√© corriendo:")
        print("   docker-compose up -d redis")
        return
    
    # Estad√≠sticas iniciales
    initial_stats = cache.get_cache_stats()
    print(f"\nüìä ESTADO INICIAL DEL CACH√â:")
    print(f"   Respuestas cacheadas: {initial_stats.get('total_cached_responses', 0)}")
    print(f"   Memoria usada: {initial_stats.get('memory_usage_mb', 0)} MB")
    print(f"   Ratio compresi√≥n: {initial_stats.get('compression_ratio', 0):.1f}%")
    
    # Test de rendimiento
    print(f"\n¬øEjecutar test de rendimiento? (s/n): ", end="")
    response = "s"  # input().strip().lower()
    
    if response == 's':
        no_cache_times, cache_times, cache_hits = test_cache_performance()
        performance = analyze_cache_performance(no_cache_times, cache_times, cache_hits)
        
        # Estad√≠sticas finales
        final_stats = cache.get_cache_stats()
        print(f"\nüìä ESTADO FINAL DEL CACH√â:")
        print(f"   Respuestas cacheadas: {final_stats.get('total_cached_responses', 0)}")
        print(f"   Memoria usada: {final_stats.get('memory_usage_mb', 0)} MB")
        print(f"   Ratio compresi√≥n: {final_stats.get('compression_ratio', 0):.1f}%")
        
        if performance:
            # Guardar resultados
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report = {
                'timestamp': timestamp,
                'performance': performance,
                'cache_stats': final_stats,
                'no_cache_times': no_cache_times,
                'cache_times': cache_times
            }
            
            with open(f'cache_optimization_{timestamp}.json', 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            print(f"\nüìÑ Resultados guardados: cache_optimization_{timestamp}.json")
            print(f"\nüéØ SIGUIENTE PASO:")
            print("T2.4: Batching simulado (15min)")
    
    else:
        print(f"\n‚è∏Ô∏è Test pospuesto.")

if __name__ == "__main__":
    main()
